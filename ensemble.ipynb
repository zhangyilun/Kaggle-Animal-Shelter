{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 level classification architecture\n",
    "Source: https://www.kaggle.com/svpons/airbnb-recruiting-new-user-bookings/three-level-classification-architecture/comments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as  np\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.base import BaseEstimator\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First ensemble technique (EN_optA)\n",
    "Given a set of predictions $X_1,X_2,...,X_n$, it computes the optimal set of weights $w_1,w_2,...,w_n$; such that minimizes $log\\_loss(y_T,y_E)$, where $y_E=\\sum_{i=1}^n X_i\\times w_i$ and $y_T$ is the true solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def objf_ens_optA(w, Xs, y, n_class=5):\n",
    "    \"\"\"\n",
    "    Function to be minimized in the EN_optA ensembler.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    w: array-like, shape=(n_preds)\n",
    "       Candidate solution to the optimization problem (vector of weights).\n",
    "    Xs: list of predictions to combine\n",
    "       Each prediction is the solution of an individual classifier and has a\n",
    "       shape=(n_samples, n_classes).\n",
    "    y: array-like sahpe=(n_samples,)\n",
    "       Class labels\n",
    "    n_class: int\n",
    "       Number of classes in the problem (5 in Animal Shelter competition)\n",
    "    \n",
    "    Return:\n",
    "    ------\n",
    "    score: Score of the candidate solution.\n",
    "    \"\"\"\n",
    "    w = np.abs(w)\n",
    "    sol = np.zeros(Xs[0].shape)\n",
    "    for i in range(len(w)):\n",
    "        sol += Xs[i] * w[i]\n",
    "    #Using log-loss as objective function (different objective functions can be used here). \n",
    "    score = log_loss(y, sol)   \n",
    "    return score\n",
    "        \n",
    "\n",
    "class EN_optA(BaseEstimator):\n",
    "    \"\"\"\n",
    "    Given a set of predictions $X_1, X_2, ..., X_n$,  it computes the optimal set of weights\n",
    "    $w_1, w_2, ..., w_n$; such that minimizes $log\\_loss(y_T, y_E)$, \n",
    "    where $y_E = X_1*w_1 + X_2*w_2 +...+ X_n*w_n$ and $y_T$ is the true solution.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_class=5):\n",
    "        super(EN_optA, self).__init__()\n",
    "        self.n_class = n_class\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Learn the optimal weights by solving an optimization problem.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be ensembled\n",
    "           Each prediction is the solution of an individual classifier and has \n",
    "           shape=(n_samples, n_classes).\n",
    "        y: array-like\n",
    "           Class labels\n",
    "        \"\"\"\n",
    "        Xs = np.hsplit(X, X.shape[1] / self.n_class)\n",
    "        #Initial solution has equal weight for all individual predictions.\n",
    "        x0 = np.ones(len(Xs)) / float(len(Xs)) \n",
    "        #Weights must be bounded in [0, 1]\n",
    "        bounds = [(0,1)]*len(x0)   \n",
    "        #All weights must sum to 1\n",
    "        cons = ({'type':'eq','fun':lambda w: 1-sum(w)})\n",
    "        #Calling the solver\n",
    "        res = minimize(objf_ens_optA, x0, args=(Xs, y, self.n_class), \n",
    "                       method='SLSQP', \n",
    "                       bounds=bounds,\n",
    "                       constraints=cons\n",
    "                       )\n",
    "        self.w = res.x\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Use the weights learned in training to predict class probabilities.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be blended.\n",
    "            Each prediction is the solution of an individual classifier and has \n",
    "            shape=(n_samples, n_classes).\n",
    "            \n",
    "        Return:\n",
    "        ------\n",
    "        y_pred: array_like, shape=(n_samples, n_class)\n",
    "                The blended prediction.\n",
    "        \"\"\"\n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        y_pred = np.zeros(Xs[0].shape)\n",
    "        for i in range(len(self.w)):\n",
    "            y_pred += Xs[i] * self.w[i] \n",
    "        return y_pred  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second ensemble technique (EN_optB)\n",
    "Given a set of predictions $X_1,X_2$, where each $X_i$ has $m=5$ classes, i.e. $X_i=X_{i1},...,X_{im}$. The algorithm finds the optimal set of weights $w_{11},...,w_{nm}$ such that minimizes $log\\_loss(y_T,y_E)$ where $y_E=\\sum_{i=1}^n\\sum_{j=1}^m X_{ij}\\times w_{ij}$ and $y_T$ is the true solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def objf_ens_optB(w, Xs, y, n_class=5):\n",
    "    \"\"\"\n",
    "    Function to be minimized in the EN_optB ensembler.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    w: array-like, shape=(n_preds)\n",
    "       Candidate solution to the optimization problem (vector of weights).\n",
    "    Xs: list of predictions to combine\n",
    "       Each prediction is the solution of an individual classifier and has a\n",
    "       shape=(n_samples, n_classes).\n",
    "    y: array-like sahpe=(n_samples,)\n",
    "       Class labels\n",
    "    n_class: int\n",
    "       Number of classes in the problem, i.e. = 5\n",
    "    \n",
    "    Return:\n",
    "    ------\n",
    "    score: Score of the candidate solution.\n",
    "    \"\"\"\n",
    "    #Constraining the weights for each class to sum up to 1.\n",
    "    #This constraint can be defined in the scipy.minimize function, but doing \n",
    "    #it here gives more flexibility to the scipy.minimize function \n",
    "    #(e.g. more solvers are allowed).\n",
    "    w_range = np.arange(len(w))%n_class \n",
    "    for i in range(n_class): \n",
    "        w[w_range==i] = w[w_range==i] / np.sum(w[w_range==i])\n",
    "        \n",
    "    sol = np.zeros(Xs[0].shape)\n",
    "    for i in range(len(w)):\n",
    "        sol[:, i % n_class] += Xs[int(i / n_class)][:, i % n_class] * w[i] \n",
    "        \n",
    "    #Using log-loss as objective function (different objective functions can be used here). \n",
    "    score = log_loss(y, sol)   \n",
    "    return score\n",
    "    \n",
    "\n",
    "class EN_optB(BaseEstimator):\n",
    "    \"\"\"\n",
    "    Given a set of predictions $X_1, X_2, ..., X_n$, where each $X_i$ has\n",
    "    $m=12$ clases, i.e. $X_i = X_{i1}, X_{i2},...,X_{im}$. The algorithm finds the optimal \n",
    "    set of weights $w_{11}, w_{12}, ..., w_{nm}$; such that minimizes \n",
    "    $log\\_loss(y_T, y_E)$, where $y_E = X_{11}*w_{11} +... + X_{21}*w_{21} + ... \n",
    "    + X_{nm}*w_{nm}$ and and $y_T$ is the true solution.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_class=5):\n",
    "        super(EN_optB, self).__init__()\n",
    "        self.n_class = n_class\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Learn the optimal weights by solving an optimization problem.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be ensembled\n",
    "           Each prediction is the solution of an individual classifier and has \n",
    "           shape=(n_samples, n_classes).\n",
    "        y: array-like\n",
    "           Class labels\n",
    "        \"\"\"\n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        #Initial solution has equal weight for all individual predictions.\n",
    "        x0 = np.ones(self.n_class * len(Xs)) / float(len(Xs)) \n",
    "        #Weights must be bounded in [0, 1]\n",
    "        bounds = [(0,1)]*len(x0)   \n",
    "        #Calling the solver (constraints are directly defined in the objective\n",
    "        #function)\n",
    "        res = minimize(objf_ens_optB, x0, args=(Xs, y, self.n_class), \n",
    "                       method='L-BFGS-B', \n",
    "                       bounds=bounds, \n",
    "                       )\n",
    "        self.w = res.x\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Use the weights learned in training to predict class probabilities.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be ensembled\n",
    "            Each prediction is the solution of an individual classifier and has \n",
    "            shape=(n_samples, n_classes).\n",
    "            \n",
    "        Return:\n",
    "        ------\n",
    "        y_pred: array_like, shape=(n_samples, n_class)\n",
    "                The ensembled prediction.\n",
    "        \"\"\"\n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        y_pred = np.zeros(Xs[0].shape)\n",
    "        for i in range(len(self.w)):\n",
    "            y_pred[:, i % self.n_class] += \\\n",
    "                   Xs[int(i / self.n_class)][:, i % self.n_class] * self.w[i]  \n",
    "        return y_pred      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#fixing random state\n",
    "random_state = 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data\n",
    "train = pd.read_csv(\"data/train_clean.csv\")\n",
    "test = pd.read_csv(\"data/test_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26728, 452)\n",
      "(26728,)\n",
      "(11456, 452)\n"
     ]
    }
   ],
   "source": [
    "# split data\n",
    "train_X = train.ix[:, train.columns != \"OutcomeType\"]\n",
    "train_Y = train[\"OutcomeType\"]\n",
    "\n",
    "mapping = {\n",
    "    \"Adoption\": 0,\n",
    "    \"Died\": 1,\n",
    "    \"Euthanasia\": 2,\n",
    "    \"Return_to_owner\": 3,\n",
    "    \"Transfer\": 4\n",
    "}\n",
    "train_Y = train_Y.replace(mapping)\n",
    "\n",
    "test_ID = test[\"ID\"]\n",
    "test_X = test.ix[:, test.columns != \"ID\"]\n",
    "\n",
    "# if using the smote data set\n",
    "test_X.columns = [x.replace(\" \",\".\") for x in test_X.columns]\n",
    "\n",
    "print(train_X.shape)\n",
    "print(train_Y.shape)\n",
    "print(test_X.shape)\n",
    "\n",
    "train_X = train_X.astype(float)\n",
    "test_X = test_X.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape:\n",
      "X_train: (16036, 452), X_valid: (5346, 452), X_test: (5346, 452) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spliting data into train and test sets.\n",
    "X, X_test, y, y_test = train_test_split(train_X, train_Y, test_size=0.2, \n",
    "                                        random_state=random_state)\n",
    "\n",
    "# spliting train data into training and validation sets.\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.25, \n",
    "                                                      random_state=random_state)\n",
    "\n",
    "print('Data shape:')\n",
    "print('X_train: %s, X_valid: %s, X_test: %s \\n' %(X_train.shape, X_valid.shape, \n",
    "                                                  X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First layer (indivisual classifiers)\n",
    "All classifiers are applied twice:\n",
    "- train on (X_train,y_train) and predicting on (X_valid)\n",
    "- train on (X,y) and predicting on (X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of individual classifiers (1st layer) on X_test\n",
      "------------------------------------------------------------\n",
      "GBM3:      logloss  => 0.7777456\n",
      "GBM1:      logloss  => 0.7828621\n",
      "GBM4:      logloss  => 0.7845642\n",
      "GBM2:      logloss  => 0.7789858\n",
      "RF:        logloss  => 0.8173097\n",
      "ETC:       logloss  => 0.8671636\n",
      "GBM5:      logloss  => 0.8118395\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define the classifiers\n",
    "clfs = {# 'LR'  : LogisticRegression(random_state=random_state), \n",
    "        # 'SVM' : SVC(probability=True, random_state=random_state), \n",
    "        'GBM1' : GradientBoostingClassifier(n_estimators=100, \n",
    "                                            learning_rate=0.1,\n",
    "                                            random_state=random_state), \n",
    "        'GBM2' : GradientBoostingClassifier(n_estimators=150, \n",
    "                                            learning_rate=0.1,\n",
    "                                            random_state=random_state), \n",
    "        'GBM3' : GradientBoostingClassifier(n_estimators=200, \n",
    "                                            learning_rate=0.1,\n",
    "                                            random_state=random_state), \n",
    "        'GBM4' : GradientBoostingClassifier(n_estimators=100, \n",
    "                                            max_depth = 6,\n",
    "                                            learning_rate=0.2,\n",
    "                                            random_state=random_state), \n",
    "        'GBM5' : GradientBoostingClassifier(n_estimators=200, \n",
    "                                            max_depth = 6,\n",
    "                                            learning_rate=0.2,\n",
    "                                            random_state=random_state), \n",
    "        'RF'  : RandomForestClassifier(n_estimators=1000, n_jobs=-1, \n",
    "                                       random_state=random_state), \n",
    "        'ETC' : ExtraTreesClassifier(n_estimators=1000, n_jobs=-1, \n",
    "                                     random_state=random_state),\n",
    "        # 'KNN' : KNeighborsClassifier(n_neighbors=30)\n",
    "       }\n",
    "    \n",
    "# predictions on the validation and test sets\n",
    "p_valid = []\n",
    "p_test = []\n",
    "   \n",
    "print('Performance of individual classifiers (1st layer) on X_test')   \n",
    "print('------------------------------------------------------------')\n",
    "   \n",
    "for nm, clf in clfs.items():\n",
    "    # first run. training on (X_train, y_train) and predicting on X_valid.\n",
    "    clf.fit(X_train, y_train)\n",
    "    yv = clf.predict_proba(X_valid)\n",
    "    p_valid.append(yv)\n",
    "        \n",
    "    # second run. training on (X, y) and predicting on X_test.\n",
    "    clf.fit(X, y)\n",
    "    yt = clf.predict_proba(X_test)\n",
    "    p_test.append(yt)\n",
    "       \n",
    "    #Printing out the performance of the classifier\n",
    "    print('{:10s} {:2s} {:1.7f}'.format('%s: ' %(nm), 'logloss  =>', log_loss(y_test, yt)))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second layer (optimization based ensembles)\n",
    "Predictions on X_valid are used as training set (XV) and predictions on X_test are used as test set (XT). EN_optA, EN_optB and their calibrated versions are applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of optimization based ensemblers (2nd layer) on X_test\n",
      "------------------------------------------------------------\n",
      "EN_optA:             logloss  => 0.7552470\n",
      "Calibrated_EN_optA:  logloss  => 0.7773318\n",
      "EN_optB:             logloss  => 0.7514325\n",
      "Calibrated_EN_optB:  logloss  => 0.7705297\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Performance of optimization based ensemblers (2nd layer) on X_test')   \n",
    "print('------------------------------------------------------------')\n",
    "    \n",
    "#Creating the data for the 2nd layer.\n",
    "XV = np.hstack(p_valid)\n",
    "XT = np.hstack(p_test)  \n",
    "        \n",
    "#EN_optA\n",
    "enA = EN_optA(5)\n",
    "enA.fit(XV, y_valid)\n",
    "w_enA = enA.w\n",
    "y_enA = enA.predict_proba(XT)\n",
    "print('{:20s} {:2s} {:1.7f}'.format('EN_optA:', 'logloss  =>', log_loss(y_test, y_enA)))\n",
    "    \n",
    "#Calibrated version of EN_optA \n",
    "cc_optA = CalibratedClassifierCV(enA, method='isotonic')\n",
    "cc_optA.fit(XV, y_valid)\n",
    "y_ccA = cc_optA.predict_proba(XT)\n",
    "print('{:20s} {:2s} {:1.7f}'.format('Calibrated_EN_optA:', 'logloss  =>', log_loss(y_test, y_ccA)))\n",
    "        \n",
    "#EN_optB\n",
    "enB = EN_optB(5) \n",
    "enB.fit(XV, y_valid)\n",
    "w_enB = enB.w\n",
    "y_enB = enB.predict_proba(XT)\n",
    "print('{:20s} {:2s} {:1.7f}'.format('EN_optB:', 'logloss  =>', log_loss(y_test, y_enB)))\n",
    "\n",
    "#Calibrated version of EN_optB\n",
    "cc_optB = CalibratedClassifierCV(enB, method='isotonic')\n",
    "cc_optB.fit(XV, y_valid)\n",
    "y_ccB = cc_optB.predict_proba(XT)  \n",
    "print('{:20s} {:2s} {:1.7f}'.format('Calibrated_EN_optB:', 'logloss  =>', log_loss(y_test, y_ccB)))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third layer (weighted average)\n",
    "Simple weighted average of the previous 4 predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3rd_layer:           logloss  => 0.7508924\n"
     ]
    }
   ],
   "source": [
    "y_3l = (y_enA * 4./9.) + (y_ccA * 2./9.) + (y_enB * 2./9.) + (y_ccB * 1./9.)\n",
    "print('{:20s} {:2s} {:1.7f}'.format('3rd_layer:', 'logloss  =>', log_loss(y_test, y_3l)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the weights of each ensemble\n",
    "In the case of EN_optA, there is a weight for each prediction and in the case of EN_optB there is a weight for each class for each prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Weights of EN_optA:\n",
      "|---------------------------------------------|\n",
      "|   GBM3 |   GBM1 |   GBM4 |   GBM2 |   RF |   ETC |   GBM5 |\n",
      "|--------+--------+--------+--------+------+-------+--------|\n",
      "|   0.42 |      0 |      0 |    0.1 | 0.18 |     0 |    0.3 |\n",
      "\n",
      "                                    Weights of EN_optB:\n",
      "|-------------------------------------------------------------------------------|\n",
      "|      |   y0 |   y1 |   y2 |   y3 |   y4 |\n",
      "|------+------+------+------+------+------|\n",
      "| GBM3 | 0.52 | 0    | 0.62 | 0.52 | 0.12 |\n",
      "| GBM1 | 0    | 0.84 | 0    | 0    | 0    |\n",
      "| GBM4 | 0.22 | 0    | 0    | 0    | 0    |\n",
      "| GBM2 | 0    | 0    | 0.12 | 0    | 0.05 |\n",
      "| RF   | 0.09 | 0.16 | 0.09 | 0.3  | 0.22 |\n",
      "| ETC  | 0    | 0    | 0    | 0    | 0    |\n",
      "| GBM5 | 0.17 | 0    | 0.17 | 0.18 | 0.61 |\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "print('               Weights of EN_optA:')\n",
    "print('|---------------------------------------------|')\n",
    "wA = np.round(w_enA, decimals=2).reshape(1,-1)\n",
    "print(tabulate(wA, headers=clfs.keys(), tablefmt=\"orgtbl\"))\n",
    "print('')\n",
    "print('                                    Weights of EN_optB:')\n",
    "print('|-------------------------------------------------------------------------------|')\n",
    "wB = np.round(w_enB.reshape((-1,5)), decimals=2)\n",
    "wB = np.hstack((np.array(list(clfs.keys()), dtype=str).reshape(-1,1), wB))\n",
    "print(tabulate(wB, headers=['y%s'%(i) for i in range(5)], tablefmt=\"orgtbl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train the architecture on all train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of individual classifiers (1st layer) on X_test\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 1st layer\n",
    "\n",
    "# predictions on the validation and test sets\n",
    "p_valid = []\n",
    "p_test = []\n",
    "   \n",
    "print('Performance of individual classifiers (1st layer) on X_test')   \n",
    "print('------------------------------------------------------------')\n",
    "   \n",
    "for nm, clf in clfs.items():\n",
    "    # first run. training on (X_train, y_train) and predicting on X_valid.\n",
    "    clf.fit(train_X, train_Y)\n",
    "    yv = clf.predict_proba(test_X)\n",
    "    p_valid.append(yv)\n",
    "        \n",
    "    # second run. training on (X, y) and predicting on X_test.\n",
    "    clf.fit(train_X, train_Y)\n",
    "    yt = clf.predict_proba(test_X)\n",
    "    p_test.append(yt)\n",
    "       \n",
    "    #Printing out the performance of the classifier\n",
    "    # print('{:10s} {:2s} {:1.7f}'.format('%s: ' %(nm), 'logloss  =>', log_loss(y_test, yt)))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
